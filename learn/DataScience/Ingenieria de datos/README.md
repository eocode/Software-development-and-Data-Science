
# Ingenier√≠a de datos <!-- omit in toc -->

> * Flujo de extracci√≥n y preparaci√≥n de los datos

## Tabla de Contenido<!-- omit in toc -->
- [Introducci√≥n](#introducci%c3%b3n)
  - [¬øQu√© es la Ciencia e Ingenier√≠a de Datos?](#%c2%bfqu%c3%a9-es-la-ciencia-e-ingenier%c3%ada-de-datos)
  - [Roles](#roles)
  - [Ambiente](#ambiente)
    - [Anaconda](#anaconda)
      - [Jupyter notebooks](#jupyter-notebooks)
    - [Google colab](#google-colab)
  - [Tipos de datos](#tipos-de-datos)
  - [Fuentes de datos](#fuentes-de-datos)
    - [Herramientas para encontrar DataSets](#herramientas-para-encontrar-datasets)
  - [ETL](#etl)
- [Web Scrapping](#web-scrapping)
  - [Etica y legalidad](#etica-y-legalidad)
  - [Requests](#requests)
  - [Beautiful Soup](#beautiful-soup)
  - [Solicitudes a la web: Requests](#solicitudes-a-la-web-requests)
  - [Page Object Patter (Patr√≥n de dise√±o)](#page-object-patter-patr%c3%b3n-de-dise%c3%b1o)
  - [Scrapping JS usando Selenium](#scrapping-js-usando-selenium)
    - [XPath CheatSheet](#xpath-cheatsheet)
  - [Scrapy](#scrapy)
  - [Proxys](#proxys)
  - [Tesseract](#tesseract)
- [Pandas](#pandas)
  - [Estructuras de datos: Series](#estructuras-de-datos-series)
  - [Estructura de datos: DataFrames](#estructura-de-datos-dataframes)
  - [√çndices y selecci√≥n](#%c3%8dndices-y-selecci%c3%b3n)
    - [Dictionary like:](#dictionary-like)
    - [Numpy like:](#numpy-like)
    - [Label based:](#label-based)
  - [Data wrangling o domado de datos](#data-wrangling-o-domado-de-datos)
  - [Trabajar con datos faltanntes](#trabajar-con-datos-faltanntes)
- [Sistemas de informaci√≥n](#sistemas-de-informaci%c3%b3n)
  - [SQLite](#sqlite)
- [Nube](#nube)
- [Introducci√≥n a las APIs](#introducci%c3%b3n-a-las-apis)
  - [¬øQu√© es una API?](#%c2%bfqu%c3%a9-es-una-api)
  - [REST](#rest)
  - [Documentaci√≥n](#documentaci%c3%b3n)
  - [JSON](#json)
  - [Spotify API](#spotify-api)

# Introducci√≥n

La ingenier√≠a de datos sigue un proceso de obtenci√≥n de datos y datasets del mundo real de diferentes fuentes y lugares. Casi siempre estos datos vienen en un formato o estructura que no esta lista para el an√°lisis adecuado.

> La ingenier√≠a de datos se preocupan principalmente por implementar los pipelines que permiten automatizar la obtenci√≥n de datos y su posterior limpieza para que otros profesionales de los datos(cient√≠ficos de datos o expertos en machine learning) puedan realizar su labor. Son la primera parte de la cadena.
> 
<div align="center">
  <img src="img/1.png">
</div>

## ¬øQu√© es la Ciencia e Ingenier√≠a de Datos?

La Ciencia de Datos es la **disciplina que se encarga de extraer conocimiento de los datos disponibles.** Casi siempre cuando te realizas una pregunta sobre datos estas fuentes se encuentran escondidas, ocultas o de dif√≠cil acceso. A nuestro alrededor hay datos en tu computadora, mesa, reloj, etc.

> Los datos est√°n por todas partes.

**La Ciencia de datos es multidisciplinaria.** A diferencia de muchos otros √°mbitos profesionales dentro del mundo de la tecnolog√≠a cuando hablamos de un cient√≠fico de datos es una persona que sabe de matem√°ticas, ingenier√≠a de software y sabe de negocios.

* **Ciencia de la computaci√≥n (Programaci√≥n)**
  * Estructura de datos
  * Algoritmos
  * Visualizaci√≥n
  * Big Data
  * Programaci√≥n
* **Estad√≠stica (Matem√°ticas)**
  * Regresiones
  * Inferencias
* **Conocimiento del dominio (Negocio)**
  * Preguntar lo correcto
  * Interpretar los resultados

> Se apoya en la Computer science, Matem√°ticas(Regresiones e Inferencias),

Tambi√©n se auxilia de:

* Bases de Datos (SQL - NoSQL)
* An√°lisis de texto y procesamiento de lenguaje natural
* An√°lisis n√∫merico de datos y minado de datos
* An√°lisis de redes
* Visualizaci√≥n de datos
* Machine learning e Inteligencia Artificial
* An√°lisis de se√±ales digitales
* An√°lisis de datos en la nube(Big Data)

## Roles

Por lo menos requiere 3 roles de datos

Existen por lo menos tres diferentes roles para tener un pipeline completo de ciencia de datos. Este curso trata sobre el primer rol:

* **Data engineer:** Se encarga de obtener los datos, Limpiarlos y estructurarlos para posterior an√°lisis, crear pipelines de an√°lisis automatizado, utilizaci√≥n de herramientas en la nube, an√°lisis descriptivo de los datos.

* **Data scientist:** Una vez tiene los datos se encarga de **generar el an√°lisis matem√°tico de ellos, encontrar las relaciones entre las variables, las correlaciones, las causas y por √∫ltimo genera los modelos predictivos y prescriptivos.**

* **Machine Learning engineer:** Se encarga de **llevar las predicciones a escala, de subirlos a la nube y all√≠ generar muchas predicciones.** Se encarga de mantener la calidad del modelo.

<div align="center">
  <img src="img/de.jpg">
</div>

## Ambiente

### Anaconda

Instalar:
https://www.anaconda.com/products/individual

```shell
conda --version
conda --help
conda list
```

Para actualizar la versi√≥n de anaconda

```shell
conda update -n base -c defaults conda
```

Para crear un nuevo ambiente virtual

```shell
conda create --name data beautifulsoup4 requests numpy pandas matplotlib yaml

conda activate data
conda env list
conda deactivate
conda remove --name data all
```

#### Jupyter notebooks

Jupyter Notebooks es un **entorno de programaci√≥n en el cual podemos mezclar ejecuci√≥n de c√≥digo en vivo, visualizaciones y a√±adir markdown.**

Jupyter Notebook tiene diferentes tipos de celdas en las cuales podemos escribir c√≥digo o markdown. Si queremos ejecutar nuestro c√≥digo hacemos ctrl + enter y si queremos ejecutar y a√±adir una nueva celda shift + enter.

> Jupyter Notebook tiene dos modalidades, la modalidad de edici√≥n y navegaci√≥n.

Para ejecutarlo necesitamos lo siguiente

```shell
jupyter notebook
```

Comandos b√°sicos para usar Jupyter

* CTRL + ENTER Ejecuta celda
* SHIFT + ENTER Nueva l√≠nea
* ESC Sale de la celda
* K y J Navegaci√≥n
* En las celdas con letra M, se pasa a modo Markdown
* Para ver la lista de comandos se necesita ejecutar P
* C, X, V

Es posible convertir notebooks a un PDF

https://nbconvert.readthedocs.io/en/latest/install.html

```shell
jupyter nbconvert your_notebook.ipynb --to pdf
```

Para mayor documentaci√≥n

https://jupyter-notebook.readthedocs.io/en/stable/index.html

### Google colab

Colab es la versi√≥n de Jupyter en la nube impulsado por Google

https://colab.research.google.com/notebooks/intro.ipynb

## Tipos de datos

Los datos vienen en muchas formas y estas formas las podemos clasificar de diferentes maneras, permiti√©ndonos poder aplicar t√©cnicas distintas a cada uno de los tipos de datos.

* Los primeros datos son los primitivos.
    * int, str, bool, float, hex, oct, datetime, objetos especiales
* Estructurados
  * SQL
  * DatawareHouses
* Semi estructurados 
  * APIs
  * Tabulares (CSV/Excel)
* No estructurados.
  * HTML
  * Texto libre
  * CVs
  * Imagenes, audio, social media
  * Datos cientificos
* Cualitativos vs cuantitativos
* Tiempo real vs historicos

Los estructurados son los m√°s f√°ciles de acceder a su informaci√≥n, los semis estructurados donde podemos usar las APIs y los No estructurados son la mayor√≠a de los datos que te vas a encontrar en tu desarrollo profesional.

## Fuentes de datos
* **Web**
  * Finanzas
  * Statups
  * Negocios
  * Clima
* **APIs**
  * Google
  * Facebook
  * Twitter
* **Logs**
* **User analytics**
  * Google analytics
* **IOT**
  * Sensores
  * Autom√≥viles
* **Datos abiertos del gobierno**

### Herramientas para encontrar DataSets

* Google Data Search
https://datasetsearch.research.google.com/

* Data.World
https://data.world/

* Kaggle
https://www.kaggle.com/

* Nasa
https://data.nasa.gov/browse

* NOAA
https://www.ncdc.noaa.gov/cdo-web/datasets

* Datos abiertos M√©xico
https://datos.gob.mx/

## ETL

ETL = Extract Transform Load

* **Extract:** Es el proceso de lectura de datos de diversas fuentes
  * Base de datos
  * CRM
  * Archivos CSV
  * Datasets p√∫blicos
* **Transform:** En este momento cuando nosotros tenemos que transformar los datos, tenemos que identificar datos faltantes o datos err√≥neos o una edad negativa. En esta etapa donde tenemos que identificar todos los problemas y solucionarlos.
  * Limpieza
  * Estructurado
  * Enriquecimiento.
* **Load:** Una vez transformados debemos insertarlos en el **data warehouse**

> Depende del tipo de soluci√≥n que se haya escogido

# Web Scrapping

Las tecnolog√≠as web en principio podemos pensarlas como el internet, pero el internet es mucho m√°s grande, es la red de redes, la forma en la que millones de computadores se conectan entre ellas para transferirse informaci√≥n.

El internet tambi√©n se compone de otros pedazos como telefon√≠a(voip), mail(pop3, imap), compartir archivos(ftp). **El internet es una red que une varias redes p√∫blicas, privadas, acad√©micas, de negocios, de gobiernos, etc.**

* **Web Scrapping** es el proceso de extracci√≥n de datos almacenados en la web.
  * Recopilar informaci√≥n almacenada en un servidor web
* **Web Crawling** es para mapear e indexar p√°ginas web para conocer su contenido, as√≠ como hace Google y varios buscadores.
  * Conocer la estructura de la web

## Etica y legalidad

Es legal... depende

* Estoy violando alguna reglamentaci√≥n local
* Estoy violando los "T√©rminos y Condiciones" del sitio
* ¬øEstoy accediendo a lugares no autorizados?
* ¬øEs legal el uso que le voy a dar a los datos?

Estados Unidos lo define de la siguiente manera

> Quienquiera que acceda a una computadora sin autorizaci√≥n o exceda la autorizaci√≥n otorgada y de este modo obtenga informaci√≥n protegida

Toma en cuenta: 
* Robots.txt
  * Define reglas de buenas pr√°cticas de scrapping
  * https://andres-dev.com/guia-completa-robots/
* Mapa del sitio

> Se responsable!

Para M√©xico Articulo 211 bis 1:
http://www.diputados.gob.mx/LeyesBiblio/pdf/9_240120.pdf

> ‚ÄúArt√≠culo 211 bis 1.- Al que sin autorizaci√≥n modifique, destruya o provoque p√©rdida de informaci√≥n
contenida en sistemas o equipos de inform√°tica protegidos por alg√∫n mecanismo de seguridad, se le
impondr√°n de seis meses a dos a√±os de prisi√≥n y de cien a trescientos d√≠as multa.
Al que sin autorizaci√≥n conozca o copie informaci√≥n contenida en sistemas o equipos de inform√°tica
protegidos por alg√∫n mecanismo de seguridad, se le impondr√°n de tres meses a un a√±o de prisi√≥n y de
cincuenta a ciento cincuenta d√≠as multa.‚Äù


<hr>

**La web espec√≠ficamente es un espacio de informaci√≥n** en el cual varios documentos(y otros recursos web) se pueden acceder a trav√©s de URLs y v√≠nculos(links). La comunicaci√≥n se da a trav√©s del protocolo HTTP.

Elementos b√°sicos de la web:

* **HTML:** nos da la estructura de la informaci√≥n. Es un lenguaje para anotar pedazos de informaci√≥n para que el navegador o otros tipos de programa puedan interpretar que tipo de informaci√≥n se encuentra ah√≠.
* **CSS:** nos permite darle colores, arreglar el texto y a√±adir diferentes elementos de presentaci√≥n.
* **Javascript:** nos permite a√±adir interactividad y c√≥mputo a nuestra web.
* **JSON:** Simplemente es una forma de transmitir datos entre servidores y clientes. Es la forma est√°ndar en las que en la web y las aplicaciones se comunican con los servidores backend.

Conceptos clave

> **Pipeline** en su definici√≥n m√°s sencilla es una serie de pasos para **convertir una entrada en algo diferente**, el automatizar estos pipelines es disminuir la intervenci√≥n humana para que se ejecuten estos pipelines.

Resumiendo web scrapping

> **Web Scrapping** es **tomar la informaci√≥n publica de un sitio web, manipularla (transformarla, analizarla) para poder obtener otra informaci√≥n**, esta transformaci√≥n y an√°lisis se hacen por medio de pipelines para descargar la informaci√≥n, revisarla y poder convertirla en informaci√≥n √∫til para lo que necesitas.

## Requests

https://requests.readthedocs.io/es/latest/

Para poder experimentar con la web necesitamos un m√©todo program√°tico para solicitar URLs y obtener HTML

> **Requests:** Nos permite **generar solicitudes a la web dentro de Python y utilizar los diferentes verbos HTTP**, normalmente utilizaremos el m√©todo **GET** porque vamos a traer datos.

* requests.get('url') para hacer una solicitud a la web y nos devolver√° un objeto response

> Todas las solicitudes HTTP tienen metadatos para que los diferentes sistemas y computadoras puedan entender de qu√© va la solicitud.

## Beautiful Soup

https://www.crummy.com/software/BeautifulSoup/bs4/doc/

En el caso de Python la **librer√≠a est√°ndar para manipular los documentos HTML se llama BeautifulSoup.**

BeautifulSoup nos ayuda a **organizar gramaticalmente(parsear) el documento HTML** para que tengamos una estructura con la cual podamos manejar y extraer informaci√≥n. **BeautifulSoup convierte el string de HTML en un √°rbol de nodos para poder manipularlo.**

Para manipularlo podemos usar los selectores CSS con **soup.select()**


## Solicitudes a la web: Requests

**Un buen Data engineer utiliza los conceptos de la ingenier√≠a de software para poder desarrollar sus programa.** En nuestro caso para poder desarrollar nos apoyaremos de un patr√≥n.

## Page Object Patter (Patr√≥n de dise√±o)

> Es un patr√≥n que consiste en esconder los queries especificos que se utilizan para manipular un documento HTML detr√°s de un objeto que representa la p√°gina web.

**Si estos queries se a√±aden directamente al c√≥digo principal, el c√≥digo se vuelve fr√°gil** y va a depender mucho de la modificaci√≥n que hagan a la web otras personas y arreglarlo se vuelve muy complicado

https://medium.com/tech-tajawal/page-object-model-pom-design-pattern-f9588630800b

<div align="center">
  <img src="img/2.png">
</div>

## Scrapping JS usando Selenium

https://selenium-python.readthedocs.io/installation.html

* Instalar los bindings de Selenium para Python. √âstos nos permitir√°n controlar un navegador desde el c√≥digo.

``pip install selenium``

``conda install -c conda-forge selenium``

``pip install webdriver-manager``

### XPath CheatSheet
http://labs.timtom.ch/library-webscraping/extras/xpath-cheatsheet.md.pdf

> Selenium se debe de usar unicamente cu√°ndo sea necesario

## Scrapy

* Scrapear en paralelo
* Trabajar con Xpath en lugar de BS4
* Limitar Requests en paralelo
* Setear demoras
* Limitar dominios

## Proxys

http://cualesmiip.com/
https://www.free-proxy-list.net/
https://github.com/Anorov/PySocks

## Tesseract

https://pypi.org/project/tesserocr/

Soluci√≥n al error RuntimeError: Failed to init API, possibly an invalid tessdata path:

Descargar la carpeta tessdata del siguiente link --> https://github.com/tesseract-ocr/tessdata/archive/master.zip
Descomprimir en el escritorio la carpeta y cambiarle el nombre a solo ‚Äútessdata‚Äù
Moverla a la ruta donde tienen instalado Python, en mi caso es C:\Program Files (x86)\Python37-32
¬°Problema solucionado! üòÑ

Otra biblioteca es OpenCV para el tratamiento de imagenes

# Pandas

**Pandas nos otorga diversas facilidades para el ‚Äú‚Äúdomado de datos‚Äù‚Äù. Nos otorga dos estructuras de datos:**

* **Series:** Es un **array** unidimensional que representa una **columna**.
* **DataFrame:** Es un **conjunto de series que forman una tabla.** Se pueden acceder a trav√©s de indices como una etiqueta(label) o pueden ser posicionales es decir 0 o √≠ndice 100. 

> Tambi√©n pueden ser rangos o slices
Estas estructuras de datos no son contenedores de datos. En Pandas **las utilizamos para transformar y enriquecer nuestros datos, manipularlos, manejar los faltantes, realizar operaciones aritm√©ticas, combinar diferentes dataframes en uno solo para obtener una nueva tabla.**

https://likegeeks.com/es/tutorial-de-python-pandas/

https://www.learnpython.org/es/Pandas%20Basics

https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf

## Estructuras de datos: Series

Series es un **vector unidimensional**, para poder acceder a esta lista **podemos usar posiciones o labels**, siendo este √∫ltimo el preferido para manipular las series. **Una diferencia importante sobre las listas de Python es que los datos son homog√©neos**, es decir solo podemos tener un tipo de dato por cada Serie.

> Las Series se pueden crear a partir de cualquier secuencia(listas, tuplas, arrays de numpy y diccionarios).

En Python tenemos la **filosof√≠a del Duck Typing, si se ve como un pato y hace cuac, a ese animal le llamamos pato,** si una serie se comporta una lista, se accede como una lista en principio deber√≠amos llamarla lista, pero esto no es as√≠.

> Una mejor aproximaci√≥n para inicializar Series es utilizar **diccionarios**.

<div align="center">
  <img src="img/4.png">
</div>

<div align="center">
  <img src="img/3.png">
</div>



## Estructura de datos: DataFrames

DataFrames **son simplemente una tabla donde las filas y las columnas tienen etiquetas**, se puede construir de diferentes formas pero siempre debemos considerar que la estructura que necesitamos construir para inicializarla tiene que ser bidimensional. 

> Una matriz y puede ser una lista de listas, lista de tuplas, un diccionario de Python u otro DataFrame.

**Si solo tenemos una dimensi√≥n a eso no le llamamos DataFrame, le llamamos Serie**. Cuando utilizamos un diccionario las llaves se convierten en las llaves de la columna.

## √çndices y selecci√≥n
Existen muchas formas de manipular los DataFrames y de seleccionar los elementos que queremos transformar.

### Dictionary like:

```python
df['col1'] 
df[['col1', 'col3']]
```

### Numpy like:

```python
iloc = index location
df.iloc[:]
df.iloc[:,:]
```

### Label based:
```python
loc = location
df.loc[:]
df.loc[:,:]
```
|
Existe una gran diferencia en la forma en la que utilizamos estos slices porque varia de la forma tradicional de Python. loc va a incluir el final del que necesitamos.


## Data wrangling o domado de datos

**Data wrangling es una de las actividades m√°s importantes de todos los profesionales de datos.** Simplemente es limpiar, transformar y enriquecer el dataset para objetivos posteriores.

Pandas es una de las herramientas m√°s poderosas para realizar este ‚Äú‚Äúdomado‚Äù‚Äù de datos. Recordemos que Pandas trae muchas de sus abstracciones del lenguaje R, pero nos otorga lo mejor de ambos mundos, por eso es tan popular.

Nos permite:

* Generar transformaciones con gran facilidad.
* Trabajar r√°pidamente con datasets grandes
* Detectar y reemplazar faltantes
agrupar y resumir nuestros datos
* Visualizar nuestros resultados.

## Trabajar con datos faltanntes

Los datos faltantes representan un verdadero problema sobre todo cuando estamos realizando agregaciones. **Imagina que tenemos datos faltantes y los llenamos con 0, pero eso har√≠a que la distribuci√≥n de datos se modificar√≠a radicalmente.** Podemos eliminar los registros, pero la fuerza de nuestras conclusiones se debilita.

Pandas nos otorga varias funcionalidades para identificarlas y para trabajar con ellas. 

**Existe el concepto que se llama NaN, cuando existe un dato faltante simplemente se rellena con un NaN y en ese momento podemos preguntar cu√°les son los datos faltantes con .isna().**

* **notna()** para preguntar d√≥nde hay datos completos.
* **dropna()** para eliminar el registro.

Para reemplazar:

* **fillna()** donde le damos un dato centinela
* **ffill()** donde utiliza el √∫ltimo valor.

https://www.geeksforgeeks.org/difference-between-map-applymap-and-apply-methods-in-pandas/

https://www.geeksforgeeks.org/how-to-get-a-list-of-class-attributes-in-python/

# Sistemas de informaci√≥n

**Los sistemas de datos vienen en muchos sabores y colores, SQL, NoSQL especializados en procesamiento en bloque, chorro y streaming.** Este tipo de sistema nos permite realizar queries sofisticadas y compartir nuestro trabajo con otros miembros del equipo.

* **Procesamiento de bloque:** Estamos hablando de datos hist√≥ricos, qu√© sucedi√≥ ayer, en el trimestre pasado, cu√°les fueron las ventas del a√±o anterior o de los √∫ltimos cinco a√±os. Nos permite realizar el procesamiento de manera eficiente.

* **Procesamiento en chorro:** Significa que estamos procesando los datos conforme van llegando, las transformaciones se realizan en tiempo real, Este tipo de sistema nos sirven para cuando queremos realizar decisiones en donde la importancia del tiempo es fundamental.

El criterio principal a tener en cuenta: **El tiempo que tienes.** Si bien los sistemas open source son gratis, para poderlos implementar necesitas tener conocimientos de cloud, debes poder saber trabajar y mantener m√°quinas.

SQL vs NoSQL

La discusi√≥n m√°s relevante en el mundo de las aplicaciones web y m√≥vil, donde dependiendo de la aplicaci√≥n, la decisi√≥n puede ser fundamental para el crecimiento de la app.
La verdad es que para los profesionales de los datos, especialmente los profesionales de los datos. Es necesario saber ambos.

## SQLite

https://www.sqlalchemy.org/

# Nube

La nube nos da un poder de c√≥mputo casi inimaginable, nos permite procesar terabytes de datos en segundos. La nube se puede usar en dos grandes ocasiones. Cuando los datos ya no caben en tu computadora loca o cuando el tiempo de procesamiento esta siendo muy extenso, es en ese momento donde deber√≠as usar la nube.

Si estas en un entorno de producci√≥n, si estas trabajando en una empresa y los datos de esa empresa ya viven en la nube, lo l√≥gico es realizar el trabajo en la nube. Automatizar los scripts en ese mismo ambiente.

Diversas nubes ya ofrecen paquetes completos para el ciclo de datos, como Google Cloud:

* Dataflow
* Pub/Sub
* Cloud Storage
* Datalab
* BigQuery
* Dataproc
* Firestore

# Introducci√≥n a las APIs

## ¬øQu√© es una API?

Por sus siglas en ingl√©s, una API es una i**nterfaz para programar aplicaciones (Application Programming Interface).** Es decir que es un conjunto de funciones, m√©todos, reglas y definiciones que nos permitir√°n desarrollar aplicaciones (en este caso un scraper) que se comuniquen con los servidores de Spotify. **Las APIs son dise√±adas y desarrolladas por las empresas que tienen inter√©s en que se desarrollen aplicaciones (p√∫blicas o privadas) que utilicen sus servicios.** 

## REST
Un t√©rmino se seguramente te vas a encontrar cuando est√©s buscando informaci√≥n en internet es REST o RESTful.**Significa representational state transfer y si una API es REST o RESTful, implica que respeta unos determinados principios de arquitectura, como por ejemplo un protocolo de comunicaci√≥n cliente/servidor (que ser√° HTTP) y (entre otras cosas) un conjunto de operaciones definidas que conocemos como m√©todos.** Ya ven√≠amos usando el m√©todo GET para hacer solicitudes a servidores web.

## Documentaci√≥n
Las APIs son dise√±adas por las mismas empresas que tienen inter√©s en que se desarrollen aplicaciones (p√∫blicas o privadas) que consuman sus servicios o informaci√≥n. Es por eso que la forma de utilizar las APIs variar√° dependiendo del servicio que querramos consumir. No es lo mismo utilizar las APIs de Spotify que las APIs de Twitter. Por esta raz√≥n es de suma importancia leer la documentaci√≥n disponible, generalmente en la secci√≥n de desarrolladores de cada sitio. 

## JSON
**Json significa JavaScript Object Notation y es un formato para describir objetos que gan√≥ tanta popularidad en su uso que ahora se lo considera independiente del lenguaje.** De hecho, lo utilizaremos en este proyecto por m√°s que estemos trabajando en Python, porque es la forma en la que obtendremos las respuestas a las solicitudes que realicemos utilizando las APIs. 

## Spotify API

https://developer.spotify.com/documentation/